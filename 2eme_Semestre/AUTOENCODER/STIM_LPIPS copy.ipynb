{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from matplotlib import pyplot as plt # type: ignore\n",
    "import cv2 # type: ignore\n",
    "from keras.metrics import MeanSquaredError # type: ignore\n",
    "from tensorflow import keras # type: ignore\n",
    "import keras_tuner as kt # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle VGG16 pré-entraîné\n",
    "vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "# Récupérer les noms des couches de convolution\n",
    "layer_names = [layer.name for layer in vgg.layers if 'conv' in layer.name]\n",
    "\n",
    "# Créer un dictionnaire pour stocker les modèles des couches de convolution\n",
    "conv_layers = {}\n",
    "\n",
    "# Stocker chacune des couches dans le dictionnaire\n",
    "for i, layer_name in enumerate(layer_names):\n",
    "    conv_layers[f'get_features_maps_{i}'] = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer(layer_name).output)\n",
    "\n",
    "def normalize(x):\n",
    "    # Fonction min-max pour les Tensors\n",
    "    min_val = tf.reduce_min(x)\n",
    "    max_val = tf.reduce_max(x)\n",
    "    return (x - min_val) / (max_val - min_val)\n",
    "\n",
    "def lpips(batch_pred, batch_true):\n",
    "    # Convertir les images en 3 canaux parce qu'elles sont en niveaux de gris\n",
    "    batch_pred = tf.image.grayscale_to_rgb(batch_pred)\n",
    "    batch_true = tf.image.grayscale_to_rgb(batch_true)\n",
    "\n",
    "    # Appliquer le prétraitement VGG16 aux images\n",
    "    batch_pred = tf.keras.applications.vgg16.preprocess_input(batch_pred)\n",
    "    batch_true = tf.keras.applications.vgg16.preprocess_input(batch_true)\n",
    "\n",
    "    # Initialiser la perte totale\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Extraire et normaliser les cartes de caractéristiques des deux images pour chaque couche de convolution\n",
    "    for key in conv_layers.keys():\n",
    "        pred_features = conv_layers[key](batch_pred)\n",
    "        true_features = conv_layers[key](batch_true)\n",
    "        total_loss += tf.reduce_mean(tf.square(pred_features - true_features))\n",
    "\n",
    "    # Normaliser la perte totale\n",
    "    total_loss /= len(conv_layers)\n",
    "\n",
    "    \"\"\" # Ajouter la différence au niveau pixel\n",
    "    pixel_diff = tf.reduce_mean(tf.square(batch_pred - batch_true))\n",
    "    total_loss += pixel_diff \"\"\"\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1, activate=True):\n",
    "    shortcut = x\n",
    "    # Convolutions\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=1, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        # Ajuste les dimensions\n",
    "        shortcut = tf.keras.layers.Conv2D(filters, 1, strides=stride, padding='same', use_bias=False)(shortcut)\n",
    "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def residual_block_recon(x, filters, kernel_size=3, stride=1, activate=True):\n",
    "    shortcut = x\n",
    "    # Première convolution\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    # Deuxième convolution\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=1, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Ajuste les dimensions\n",
    "    shortcut = tf.keras.layers.Conv2DTranspose(filters, 1, strides=stride, padding='same', use_bias=False)(shortcut)\n",
    "    shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_to_array(generator):\n",
    "    # Initialiser une liste pour stocker les échantillons\n",
    "    num_samples = len(generator)\n",
    "    samples = []\n",
    "    # Itérer sur le générateur pour obtenir les échantillons\n",
    "    for i in range(num_samples):\n",
    "        batch = generator.next()\n",
    "        for image in batch[0]:\n",
    "            samples.append(image)  # Ajouter uniquement les données (ignorer les étiquettes)\n",
    "    return np.array(samples)\n",
    "\n",
    "def comparaison_visages(asian, asian_predict, white, white_predict, ethnie):\n",
    "\n",
    "    num_images = 6\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(num_images):\n",
    "        # Afficher l'image originale\n",
    "        ax = plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(cv2.resize(white[i],(224, 224), interpolation=cv2.INTER_LINEAR), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
    "        plt.imshow(cv2.resize(white_predict[i], (224, 224), interpolation=cv2.INTER_LINEAR), cmap='gray')\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(num_images):\n",
    "        # Afficher l'image originale\n",
    "        ax = plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(cv2.resize(asian[i],(224, 224), interpolation=cv2.INTER_LINEAR), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
    "        plt.imshow(cv2.resize(asian_predict[i], (224, 224), interpolation=cv2.INTER_LINEAR), cmap='gray')\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "trainset_white = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Train',\n",
    "    classes=['Caucasiens'],\n",
    "    target_size=(224, 224), \n",
    "    batch_size=4, \n",
    "    class_mode='input',\n",
    "    color_mode='grayscale')\n",
    "testset_white = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Test',\n",
    "    classes=['Caucasiens'],\n",
    "    target_size=(224, 224), \n",
    "    batch_size=4, \n",
    "    class_mode='input',\n",
    "    color_mode='grayscale')\n",
    "trainset_asian = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Train',\n",
    "    classes=['Asiatiques'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=4,\n",
    "    class_mode='input',\n",
    "    color_mode='grayscale')\n",
    "testset_asian = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Test',\n",
    "    classes=['Asiatiques'],\n",
    "    target_size=(224, 224),\n",
    "    batch_size=4,\n",
    "    class_mode='input',\n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = generator_to_array(trainset_white)\n",
    "var = 0\n",
    "for i in range(50):\n",
    "    var += white[i].var()\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asian = generator_to_array(trainset_asian)\n",
    "var = 0\n",
    "for i in range(50):\n",
    "    var += asian[i].var()\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, shape=(224, 224, 1), input_latent=64):\n",
    "    input_img = tf.keras.Input(shape=shape)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', use_bias=False)(input_img)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')(x)\n",
    "\n",
    "    # Encodeur\n",
    "    num_blocks = hp.Int('num_residual_blocks', min_value=3, max_value=7, step=2)\n",
    "    filters = [64, 128, 256, 512, 1024, 2048, 250]\n",
    "    strides = [2, 2, 2, 2, 2, 2, 1]\n",
    "    for i in range(num_blocks):\n",
    "        x = residual_block(x, filters[i % len(filters)], 3, strides[i % len(strides)])\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    latent_space_layer = tf.keras.layers.Dense(input_latent, activation='relu', use_bias=False)(x)\n",
    "    latent_space_layer_norm = tf.keras.layers.BatchNormalization(name='latent_space_layer_norm')(latent_space_layer)\n",
    "\n",
    "    # Décodeur\n",
    "    reshape_layer = tf.keras.layers.Reshape(target_shape=(1, 1, input_latent))(latent_space_layer_norm)\n",
    "    filters_decoder = filters[:num_blocks][::-1]  # Inverser les filtres utilisés dans l'encodeur\n",
    "    strides_decoder = strides[:num_blocks][::-1]  # Inverser les strides utilisés dans l'encodeur\n",
    "\n",
    "    x_recon = reshape_layer\n",
    "    for i in range(num_blocks):\n",
    "        x_recon = residual_block_recon(x_recon, filters_decoder[i], 3, strides_decoder[i])\n",
    "\n",
    "    x_recon = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same', use_bias=False)(x_recon)\n",
    "    x_recon = tf.keras.layers.BatchNormalization()(x_recon)\n",
    "    x_recon = tf.keras.layers.Activation('relu')(x_recon)\n",
    "    x_recon = tf.keras.layers.UpSampling2D(size=(2, 2))(x_recon)\n",
    "\n",
    "    x_recon = tf.keras.layers.Conv2DTranspose(1, 1, activation='sigmoid', padding='same', use_bias=False)(x_recon)\n",
    "    x_recon = tf.keras.layers.Resizing(224, 224)(x_recon)\n",
    "    model = tf.keras.Model(inputs=input_img, outputs=[x_recon])\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "        )\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(\n",
    "            learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "        )\n",
    "    elif optimizer_choice == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(\n",
    "            learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "        )\n",
    "    model.compile(optimizer=optimizer, loss=lpips, metrics=MeanSquaredError())\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective('val_MeanSquaredError', direction='min'),  # Objectif de minimiser la métrique SSIM\n",
    "    max_trials=20,  # Ajustez ce nombre en fonction de vos besoins\n",
    "    executions_per_trial=1,  # Nombre d'exécutions par essai pour obtenir une moyenne\n",
    "    directory='my_dir',\n",
    "    project_name='residual_blocks_lpips'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(trainset_white, epochs=10, validation_data=testset_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(trainset_white, epochs=1000, validation_data=testset_white)\n",
    "best_model.save('Modeles/STIM/best_model_white.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_asian = generator_to_array(testset_asian).reshape(6, 224, 224)\n",
    "test_asian_predict = best_model.predict(testset_asian).reshape(6, 224, 224)\n",
    "test_white = generator_to_array(testset_white).reshape(6, 224, 224)\n",
    "test_white_predict = best_model.predict(testset_white).reshape(6, 224, 224)\n",
    "comparaison_visages(test_asian, test_asian_predict, test_white, test_white_predict, \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history2 = best_model.fit(trainset_asian, epochs=1000, validation_data=testset_asian)\n",
    "best_model.save('Modeles/STIM/best_model_asian.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_asian = generator_to_array(testset_asian).reshape(6, 224, 224)\n",
    "test_asian_predict = model.predict(testset_asian).reshape(6, 224, 224)\n",
    "test_white = generator_to_array(testset_white).reshape(6, 224, 224)\n",
    "test_white_predict = model.predict(testset_white).reshape(6, 224, 224)\n",
    "comparaison_visages(test_asian, test_asian_predict, test_white, test_white_predict, \"white\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

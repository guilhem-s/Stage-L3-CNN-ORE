{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf # type: ignore\n",
    "import os\n",
    "from tensorflow.keras.models import Model, Sequential, load_model # type: ignore\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Reshape, Conv2DTranspose, Add, LeakyReLU, UpSampling2D, Dropout, Concatenate, AveragePooling2D, GlobalMaxPooling2D, Lambda, ZeroPadding2D  # type: ignore\n",
    "from keras.initializers import glorot_uniform # type: ignore\n",
    "from tensorflow.keras.optimizers import SGD, Adam # type: ignore\n",
    "from tensorflow.keras.regularizers import l2 # type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay # type: ignore\n",
    "from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy, MeanAbsoluteError # type: ignore\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "import keras_tuner as kt # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import scipy\n",
    "from numba import cuda\n",
    "from multiprocessing import Pool, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui genere des vecteurs aleatoires\n",
    "def generate_unique_vectors(num_vectors, vector_length, vectors):\n",
    "    vectors_list = []\n",
    "    while len(vectors_list) < num_vectors:\n",
    "        vector = tuple(np.random.randint(0, 2, vector_length))\n",
    "        if vector not in vectors:\n",
    "            vectors.add(vector)\n",
    "            vectors_list.append(vector)\n",
    "    return vectors_list, vectors\n",
    "\n",
    "# Fonction qui convertie le generateur en tableau numpy\n",
    "def generator_to_array(generator, class_vectors):\n",
    "    samples = []\n",
    "    vectors = []\n",
    "    data_filenames = generator.filenames\n",
    "    total_images = len(data_filenames)\n",
    "\n",
    "    for i in range(len(generator)):\n",
    "        batch = next(generator)\n",
    "        batch_size = len(batch[0])\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            index = i * generator.batch_size + j\n",
    "            if index >= total_images:\n",
    "                break  # Prevent going out of bounds\n",
    "\n",
    "            samples.append(batch[0][j])\n",
    "            class_name = data_filenames[index].split(os.path.sep)[0]\n",
    "            vectors.append(class_vectors[class_name])\n",
    "            \n",
    "    return np.array(samples), np.array(vectors), data_filenames\n",
    "\n",
    "# Fonction qui associe les images aux labels\n",
    "def preprocess(train_generator, val_generator, num_classes, vector_length, total_vectors, use_random_vectors):\n",
    "    class_indices = train_generator.class_indices\n",
    "    \n",
    "    if use_random_vectors:\n",
    "        unique_vectors, total_vectors = generate_unique_vectors(num_classes, vector_length, total_vectors)\n",
    "        class_vectors = {class_name: vector for class_name, vector in zip(class_indices, unique_vectors)}\n",
    "    else:\n",
    "        class_vectors = {class_name: i for i, class_name in enumerate(class_indices)}\n",
    "    \n",
    "    samples_train, vectors_train, _ = generator_to_array(train_generator, class_vectors)\n",
    "    samples_val, vectors_val, _ = generator_to_array(val_generator, class_vectors)\n",
    "    \n",
    "    if not use_random_vectors:\n",
    "        # Convert class indices to one-hot encoding\n",
    "        vectors_train = to_categorical(vectors_train, num_classes=num_classes)\n",
    "        vectors_val = to_categorical(vectors_val, num_classes=num_classes)\n",
    "    \n",
    "    return samples_train, vectors_train, samples_val, vectors_val, total_vectors\n",
    "\n",
    "# Define the worker function\n",
    "def process_ethnie(ethnie, target_size, batch_size, class_mode, shuffle, color_mode, use_random_vectors, vector_length):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.05, dtype='float16')\n",
    "    \n",
    "    trainset = datagen.flow_from_directory(f'../../Datasets/VGG/{ethnie}', target_size=target_size, batch_size=batch_size, class_mode=class_mode, shuffle=shuffle, color_mode=color_mode, subset='training')\n",
    "    testset  = datagen.flow_from_directory(f'../../Datasets/VGG/{ethnie}', target_size=target_size, batch_size=batch_size, class_mode=class_mode, shuffle=shuffle, color_mode=color_mode, subset='validation')\n",
    "    \n",
    "    num_classes = len(trainset.class_indices)\n",
    "    total_vectors = set()\n",
    "    \n",
    "    samples_train, vectors_train, samples_val, vectors_val, total_vectors = preprocess(trainset, testset, num_classes, vector_length, total_vectors, use_random_vectors)\n",
    "    \n",
    "    return ethnie, samples_train, vectors_train, samples_val, vectors_val\n",
    "\n",
    "# Fonction qui charge les donnees\n",
    "def load_data(target_size=(150, 150), batch_size=112, class_mode='input', shuffle=False, color_mode='grayscale', use_random_vectors=True, vector_length=56):\n",
    "    ethnies = ['caucasians', 'afro_americans', 'asians']\n",
    "    \n",
    "    # Use Manager to create a shared dictionary\n",
    "    manager = Manager()\n",
    "    return_dict = manager.dict()\n",
    "    \n",
    "    # Create a pool of processes\n",
    "    pool = Pool(processes=4*len(ethnies))\n",
    "    \n",
    "    # Create a list of arguments for each ethnic group\n",
    "    args_list = [(ethnie, target_size, batch_size, class_mode, shuffle, color_mode, use_random_vectors, vector_length) for ethnie in ethnies]\n",
    "    \n",
    "    # Map the worker function to the list of arguments\n",
    "    results = pool.starmap(process_ethnie, args_list)\n",
    "    \n",
    "    # Close the pool and wait for the work to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Collect the results from the pool\n",
    "    ethnies_data = {ethnie: [samples_train, vectors_train, samples_val, vectors_val] for ethnie, samples_train, vectors_train, samples_val, vectors_val in results}\n",
    "    \n",
    "    return ethnies_data\n",
    "\n",
    "# Parameters\n",
    "target_size = (300, 300);batch_size = 128;class_mode = 'input';shuffle = False;color_mode = 'grayscale';use_random_vectors = False;vector_length = 56\n",
    "\n",
    "# Load data\n",
    "ethnies = load_data(target_size=target_size, batch_size=batch_size, class_mode=class_mode, shuffle=shuffle, color_mode=color_mode, use_random_vectors=use_random_vectors, vector_length=vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(inputs, cardinality):\n",
    "    inputs_channels = inputs.shape[3]\n",
    "    group_size = inputs_channels // cardinality    \n",
    "    groups = list()\n",
    "    for number in range(1, cardinality+1):\n",
    "        begin = int((number-1)*group_size)\n",
    "        end = int(number*group_size)\n",
    "        block = Lambda(lambda x:x[:,:,:,begin:end])(inputs)\n",
    "        groups.append(block)\n",
    "    return groups\n",
    "\n",
    "def transform(groups, filters, stage, block, downsampling):\n",
    "    f1, f2 = filters    \n",
    "    conv_name = \"conv2d-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    bn_name = \"batchnorm-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    \n",
    "    transformed_tensor = list()\n",
    "    i = 1\n",
    "    \n",
    "    for inputs in groups:\n",
    "        # first conv of the transformation phase\n",
    "        x = Conv2D(filters=f1, kernel_size=(1, 1), padding=\"valid\", \n",
    "                   kernel_initializer=glorot_uniform(seed=0))(inputs) #name=conv_name+'1a_split'+str(i), \n",
    "        if downsampling:\n",
    "            x = MaxPooling2D(2)(x)\n",
    "        x = BatchNormalization(axis=3)(x) #name=bn_name+'1a_split'+str(i)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # second conv of the transformation phase\n",
    "        x = Conv2D(filters=f2, kernel_size=(2, 2), padding=\"same\", kernel_initializer=glorot_uniform(seed=0))(x) #name=conv_name+'1b_split'+str(i), \n",
    "        x = BatchNormalization(axis=3)(x) #name=bn_name+'1b_split'+str(i)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # Add x to transformed tensor list\n",
    "        transformed_tensor.append(x)\n",
    "        i+=1\n",
    "        \n",
    "    # Concatenate all tensor from each group\n",
    "    x = Concatenate()(transformed_tensor)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def transition(inputs, filters, stage, block):\n",
    "    x = Conv2D(filters=filters, kernel_size=(1, 1), padding=\"valid\", kernel_initializer=glorot_uniform(seed=0))(inputs) #name='conv2d-trans'+str(stage)+''+block, \n",
    "    x = BatchNormalization(axis=3)(x) #name='batchnorm-trans'+str(stage)+''+block\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def downsampling(inputs, filters, cardinality, strides, stage, block):    \n",
    "    # useful variables\n",
    "    conv_name = \"conv2d-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    bn_name = \"batchnorm-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    \n",
    "    # Retrieve filters for each layer\n",
    "    f1, f2, f3 = filters\n",
    "    \n",
    "    # save the input tensor value\n",
    "    x_shortcut = inputs\n",
    "    x = inputs\n",
    "    \n",
    "    # divide input channels into groups. The number of groups is define by cardinality param\n",
    "    groups = split(inputs=x, cardinality=cardinality)\n",
    "    \n",
    "    # transform each group by doing a set of convolutions and concat the results\n",
    "    f1 = int(f1 / cardinality)\n",
    "    f2 = int(f2 / cardinality)\n",
    "    x = transform(groups=groups, filters=(f1, f2), stage=stage, block=block, downsampling=True)\n",
    "    \n",
    "    # make a transition by doing 1x1 conv\n",
    "    x = transition(inputs=x, filters=f3, stage=stage, block=block)\n",
    "\n",
    "    # Projection Shortcut to match dimensions \n",
    "    x_shortcut = Conv2D(filters=f3, kernel_size=(1, 1), padding=\"valid\", kernel_initializer=glorot_uniform(seed=0))(x_shortcut) #name='{base}2'.format(base=conv_name),\n",
    "    x_shortcut = MaxPooling2D(2)(x_shortcut)\n",
    "    x_shortcut = BatchNormalization(axis=3)(x_shortcut) #, name='{base}2'.format(base=bn_name)\n",
    "    \n",
    "    # Add x and x_shortcut\n",
    "    x = Add()([x,x_shortcut])\n",
    "    #print(x.shape)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def transform_decoder(groups, filters, strides, stage, block):\n",
    "    conv_name = \"transpo_conv2d-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    bn_name = \"transpo_batchnorm-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    \n",
    "    transformed_tensor = []\n",
    "    f1, f2 = filters\n",
    "    i = 1\n",
    "    \n",
    "    for inputs in groups:\n",
    "        # first conv transpose of the transformation phase\n",
    "        x = Conv2DTranspose(filters=f1, kernel_size=(1,1), strides=(1,1), padding=\"same\", \n",
    "                            kernel_initializer=glorot_uniform(seed=0))(inputs) #name=conv_name+'2a_split'+str(i),\n",
    "        x = BatchNormalization(axis=3)(x) #name=bn_name+'2a_split'+str(i)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # second conv transpose of the transformation phase\n",
    "        x = Conv2DTranspose(filters=f2, kernel_size=(2, 2), strides=strides, padding=\"same\", \n",
    "                            kernel_initializer=glorot_uniform(seed=0))(x) #name=conv_name+'2b_split'+str(i), \n",
    "        x = BatchNormalization(axis=3)(x) #name=bn_name+'2b_split'+str(i)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        transformed_tensor.append(x)\n",
    "        i += 1\n",
    "        \n",
    "    x = Concatenate()(transformed_tensor)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def transition_decoder(inputs, filters, stage, block):\n",
    "    conv_name = \"transpo_conv2d-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    bn_name = \"transpo_batchnorm-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    \n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(1, 1), padding=\"valid\", \n",
    "                        kernel_initializer=glorot_uniform(seed=0))(inputs) #name=conv_name+'2',\n",
    "    x = BatchNormalization(axis=3)(x) #, name=bn_name+'2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def upsampling(inputs, filters, cardinality, strides, stage, block):    \n",
    "    # Variables utiles\n",
    "    conv_name = \"transpo_conv2d-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    bn_name = \"transpo_batchnorm-{stage}{block}-branch\".format(stage=str(stage), block=str(block))\n",
    "    \n",
    "    # Récupérer les filtres pour chaque couche\n",
    "    f1, f2, f3 = filters\n",
    "    \n",
    "    # Sauvegarder la valeur du tenseur d'entrée\n",
    "    x_shortcut = inputs\n",
    "    x = inputs\n",
    "    \n",
    "    # Diviser les canaux d'entrée en groupes. Le nombre de groupes est défini par le paramètre cardinalité\n",
    "    groups = split(inputs=x, cardinality=cardinality)\n",
    "    \n",
    "    # Transformer chaque groupe en faisant un ensemble de convolutions transposées et concaténer les résultats\n",
    "    f1 = int(f1 / cardinality)\n",
    "    f2 = int(f2 / cardinality)\n",
    "    x = transform_decoder(groups=groups, filters=(f1, f2), strides=strides, stage=stage, block=block)\n",
    "    \n",
    "    # Faire une transition en utilisant 1x1 conv transposée\n",
    "    x = transition_decoder(inputs=x, filters=f3, stage=stage, block=block)\n",
    "    # Projection du raccourci pour correspondre aux dimensions\n",
    "    x_shortcut = Conv2DTranspose(filters=f3, kernel_size=(2, 2), strides=strides, padding=\"valid\", kernel_initializer=glorot_uniform(seed=0))(x_shortcut) #name='{base}2'.format(base=conv_name), \n",
    "    x_shortcut = BatchNormalization(axis=3)(x_shortcut) #, name='{base}2'.format(base=bn_name))(x_shortcut)\n",
    "    # Ajouter x et x_shortcut\n",
    "    x = Add()([x, x_shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(input_shape, input_latent):\n",
    "    \n",
    "    filters = [128, 256, 512]\n",
    "    autoencoder=Sequential()\n",
    "\n",
    "    # Encoder\n",
    "    for i, filter in enumerate(filters):\n",
    "        autoencoder.add(Conv2D(filter, (3, 3), activation='relu', padding='same'))        \n",
    "        autoencoder.add(LeakyReLU(alpha=0.1))\n",
    "        autoencoder.add(BatchNormalization())\n",
    "        autoencoder.add(MaxPooling2D((2, 2), padding='same'))\n",
    "        print(f'After {i+1} Conv2D: {autoencoder.output_shape}')\n",
    "\n",
    "    # Espace latent\n",
    "    #x = Flatten()(x)\n",
    "    autoencoder.add(Dense(input_latent, activation='relu', name='latent_space'))\n",
    "    autoencoder.add(BatchNormalization(name='latent_space_norm'))\n",
    "\n",
    "    # Décodeur\n",
    "    for i, filter in enumerate(reversed(filters)):\n",
    "        autoencoder.add(Conv2D(filter, (3, 3), activation='relu', padding='same'))        \n",
    "        autoencoder.add(LeakyReLU(alpha=0.1))\n",
    "        autoencoder.add(BatchNormalization())\n",
    "        autoencoder.add(UpSampling2D((2, 2)))\n",
    "        print(f'After {i+1} UpSampling2D: {autoencoder.output_shape}')\n",
    "    \n",
    "    autoencoder.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
    "    print(f'Decoded shape before resizing: {autoencoder.output_shape}')    \n",
    "    # Resize to match input shape\n",
    "    autoencoder.add(Resizing(height=input_shape[0], width=input_shape[1], name='final_output'))\n",
    "    print(f'Decoded shape after resizing: {autoencoder.output_shape}')\n",
    "\n",
    "    # Autoencoder autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=MeanAbsoluteError(), metrics=['accuracy'])\n",
    "    \n",
    "    # Display the model summary\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNeXt50_AutoEncoder(input_shape, input_latent, cardinality = 64):\n",
    "    x_input = Input(input_shape, dtype='float16')\n",
    "    x = ZeroPadding2D((3, 3))(x_input)\n",
    "    x = Conv2D(filters=64, kernel_size=(5, 5), kernel_initializer='glorot_uniform')(x)\n",
    "    x = BatchNormalization(axis=3, name='batchnorm_1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    filters = [128, 128, 256, 256, 512, 1024, 2048]\n",
    "    \n",
    "    for filter in filters:\n",
    "        x = downsampling(inputs=x, filters=(filter, filter, filter*2), cardinality=cardinality, strides=(2, 2), stage=1, block=\"a\")\n",
    "        print(x.shape)\n",
    "    \n",
    "    x = Conv2D(input_latent, 2, padding='same', use_bias=False)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    latent_space_layer = Dense(input_latent, activation='relu', use_bias=False)(x)\n",
    "    latent_space_layer_norm = BatchNormalization(name='latent_space_layer_norm')(latent_space_layer)\n",
    "\n",
    "    reshape_layer = Reshape(target_shape=(1, 1, input_latent))(latent_space_layer_norm)\n",
    "    x_recon = Conv2DTranspose(input_latent, 3, strides=1, padding='same', use_bias=False)(reshape_layer)\n",
    "\n",
    "    filters.insert(0, 64)\n",
    "    for filter in reversed(filters):\n",
    "        x_recon = upsampling(inputs=x_recon, filters=(filter*2, filter*2, filter), cardinality=cardinality, strides=(2, 2), stage=5, block=\"a\")\n",
    "        print(x_recon.shape)\n",
    "    \n",
    "    x_recon = Conv2DTranspose(1, (1, 1), padding='same', activation='sigmoid')(x_recon)\n",
    "    x_recon = tf.keras.layers.Resizing(height=input_shape[0], width=input_shape[1], name='recon_image')(x_recon)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=x_recon, name=\"resnext50_autoencoder\")\n",
    "    \n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
    "    optimizer = SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "# Utilisation du modèle\n",
    "input_shape = (300, 300, 1)\n",
    "input_latent = 512\n",
    "cardinality = 32\n",
    "with strategy.scope():    \n",
    "    model = ResNeXt50_AutoEncoder(input_shape, input_latent, cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Results/weights/Resnext_best_weights_64.h5')    \n",
    "total_params = model.count_params()\n",
    "total_bytes = (total_params * 16) // 8  # En float16 (16 bits par paramètre)\n",
    "print(f\"Taille totale du modèle en octets : {total_bytes} octets\")\n",
    "\n",
    "# Data Preparation\n",
    "ethnie = 'caucasians'\n",
    "x_train = ethnies[ethnie][0]\n",
    "x_val = ethnies[ethnie][2]\n",
    "x_print = x_val.copy()\n",
    "np.random.shuffle(x_print)\n",
    "x_print = x_print[:10]\n",
    "\n",
    "def create_dataset(data, batch_size):\n",
    "    # Convertir les données en float16\n",
    "    data = tf.cast(data, tf.float16)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, data))  # Utilisation des données comme input et target\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(x_train, 128)\n",
    "val_dataset = create_dataset(x_val, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data, display_loss_interval=10, display_recon_interval=20, save_weight_interval=20):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.display_loss_interval = display_loss_interval\n",
    "        self.display_recon_interval = display_recon_interval\n",
    "        self.save_weight_interval = save_weight_interval\n",
    "\n",
    "        # Initialisation du callback ModelCheckpoint\n",
    "        self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='Results/weights/Resnext_best_weights_32.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            save_weights_only=True,\n",
    "            save_freq='epoch'\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.display_recon_interval == 0:\n",
    "            reconstructions = self.model.predict(self.test_data, verbose=0)\n",
    "            self.display_reconstruction(epoch+1, self.test_data, reconstructions)\n",
    "        if epoch % self.display_loss_interval == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {logs['loss']:.4g}\")\n",
    "        if epoch % self.save_weight_interval == 0:\n",
    "            # Appel du callback ModelCheckpoint\n",
    "            self.checkpoint_callback.set_model(self.model)\n",
    "            self.checkpoint_callback.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "    def display_reconstruction(self, epoch, originals, reconstructions):\n",
    "        n = 10  # Nombre d'images à afficher\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        for i in range(n):\n",
    "            # Affichage original\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "            plt.imshow(originals[i].reshape(200, 200), cmap='gray')\n",
    "            plt.title(\"Original\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Affichage de la reconstruction\n",
    "            ax = plt.subplot(2, n, i + 1 + n)\n",
    "            plt.imshow(reconstructions[i].reshape(200, 200), cmap='gray')\n",
    "            plt.title(\"Reconstructed\")\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f'Epoch {epoch}')\n",
    "        plt.savefig(f'Results/MSE/Resnext2_{epoch}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Création de l'instance du callback\n",
    "callback = CustomCallback(x_print)\n",
    "model.fit(train_dataset, epochs=801, validation_data=val_dataset, callbacks=callback, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envguilhem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

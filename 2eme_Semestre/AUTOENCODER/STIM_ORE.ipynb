{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_to_array(generator):\n",
    "    # Initialiser une liste pour stocker les échantillons\n",
    "    num_samples = len(generator)\n",
    "    samples = []\n",
    "    # Itérer sur le générateur pour obtenir les échantillons\n",
    "    for i in range(num_samples):\n",
    "        batch = generator.next()\n",
    "        for image in batch[0]:\n",
    "            samples.append(image)  # Ajouter uniquement les données (ignorer les étiquettes)\n",
    "    return np.array(samples)\n",
    "\n",
    "def mean_cosinus_similarity(v1, v2):\n",
    "    cosinus_similarity = 0\n",
    "    nb_individus = v1.shape[0]\n",
    "    for i in range(nb_individus):\n",
    "        cosinus_similarity += np.dot(v1[i], v2[i]) / (np.linalg.norm(v1[i]) * np.linalg.norm(v2[i]))\n",
    "    return cosinus_similarity/nb_individus\n",
    "\n",
    "def calculate_mean_dispersion(latent_representations):\n",
    "    centroid = np.mean(latent_representations, axis=0)\n",
    "    distance_squared = np.sum((latent_representations - centroid)**2, axis = 1)\n",
    "    return np.mean(distance_squared)\n",
    "\n",
    "def comparaison_visages(asian, asian_predict, white, white_predict, ethnie):\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Affichage des images du premier trainset\n",
    "    plt.subplot(2, 5, (1, 5))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('Caucasiens')\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(white[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.imshow(white_predict[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Titre global\n",
    "    plt.suptitle(f'Comparaison des images d entrées et de sorties caucasiennes d espace latent {ethnie}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Affichage des images du premier trainset\n",
    "    plt.subplot(2, 5, (1, 5))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('Asiatiques')\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(asian[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.imshow(asian_predict[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Titre global\n",
    "    plt.suptitle('Comparaison des images d entrées et de sorties asiatiques')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def pca(latent_asian, latent_white, ethnie):\n",
    "    pca = PCA(n_components=2)\n",
    "    latent_pca_white = pca.fit_transform(latent_white)\n",
    "    latent_pca_asian = pca.fit_transform(latent_asian)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(latent_pca_white[:, 0], latent_pca_white[:, 1], color='r', alpha=0.5, label='white_predict')\n",
    "    scatter_b = plt.scatter(latent_pca_asian[:, 0], latent_pca_asian[:, 1], color='b', alpha=0.5, label='asian_predict')\n",
    "    plt.legend(handles=[scatter, scatter_b])\n",
    "    plt.title(f'Projection ACP de l espace latent {ethnie}')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()\n",
    "\n",
    "    return np.concatenate([latent_pca_white, latent_pca_asian], axis=0)\n",
    "\n",
    "\n",
    "def cosinus_similarity(asian_flatten, asian_predict_flatten, white_flatten, white_predict_flatten):\n",
    "    asian_cs = mean_cosinus_similarity(asian_flatten, asian_predict_flatten)\n",
    "    white_cs = mean_cosinus_similarity(white_flatten, white_predict_flatten)\n",
    "    print(f\"Moyenne des similarités cosinus pour les individus caucasiens : {white_cs}\")\n",
    "    print(f\"Moyenne des similarités cosinus pour les individus asiatiques : {asian_cs}\")\n",
    "    \n",
    "def comparaison_dispersion(latent_asian, latent_white):\n",
    "    asian_mean_dispersion = calculate_mean_dispersion(latent_asian)\n",
    "    white_mean_dispersion = calculate_mean_dispersion(latent_white)\n",
    "    print(f\"Moyenne des dispersions des espaces latents pour les individus caucasiens : {white_mean_dispersion}\")\n",
    "    print(f\"Moyenne des dispersions des espaces latents pour les individus asiatiques : {asian_mean_dispersion}\")\n",
    "\n",
    "def coef_bhattacharyya(latent_asian, latent_white):\n",
    "    normalize_a = tf.nn.l2_normalize(np.mean(latent_asian, axis=0),axis=0)        \n",
    "    normalize_b = tf.nn.l2_normalize(np.mean(latent_white, axis=0),axis=0)\n",
    "    return tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
    "\n",
    "def ssim_mean(ytrue, ypred, length):    \n",
    "    res = 0\n",
    "    for i in range(length):\n",
    "        res+=ssim(ytrue[i], ypred[i], data_range=ytrue[i].max() - ytrue[i].min(), multichannel=False)\n",
    "    \n",
    "    return res/length\n",
    "\n",
    "def structure_ssim(ytrue, ypred):\n",
    "    return ssim(ytrue, ypred, data_range=ytrue.max() - ytrue.min(), multichannel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, dtype='float16')\n",
    "datatrain_white = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Train',\n",
    "    classes=['Caucasiens'],\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64, \n",
    "    class_mode='input',\n",
    "    shuffle=True,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "datatrain_east_asian = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Train',\n",
    "    classes=['Asiatiques'],\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='input',\n",
    "    shuffle=True,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "datatest_white = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Test',\n",
    "    classes=['Caucasiens'],\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64, \n",
    "    class_mode='input',\n",
    "    shuffle=True,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "datatest_east_asian = train_datagen.flow_from_directory(\n",
    "    '../../Datasets/STIM_NB_LumNorm/Test',\n",
    "    classes=['Asiatiques'],\n",
    "    target_size=(150, 150),\n",
    "    batch_size=64,\n",
    "    class_mode='input',\n",
    "    shuffle=True,\n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModeles/STIM/stim_autoencoder_white_32.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('Modeles/STIM/stim_autoencoder_white_32.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie = 'white'\n",
    "get_latent = keras.Model(inputs=model.input, outputs=model.get_layer('latent_space_layer_norm').output)\n",
    "latent_asian = get_latent.predict(datatrain_east_asian)\n",
    "latent_white = get_latent.predict(datatrain_white)\n",
    "\n",
    "asian_predict = model.predict(datatrain_east_asian).reshape(50,150,150)\n",
    "white_predict = model.predict(datatrain_white).reshape(50,150,150)\n",
    "asian = generator_to_array(datatrain_east_asian).reshape(50,150,150)\n",
    "white = generator_to_array(datatrain_white).reshape(50,150,150)\n",
    "\n",
    "asian_predict_flatten = asian_predict.reshape(50, -1)\n",
    "white_predict_flatten = white_predict.reshape(50, -1)\n",
    "asian_flatten = asian.reshape(50, -1)\n",
    "white_flatten = white.reshape(50, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_visages(asian, asian_predict, white, white_predict, \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(asian, asian_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ssim_mean(asian, asian_predict, 50))\n",
    "ssim_mean(white, white_predict, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(asian, asian_predict, data_range=asian.max() - asian.min(), multichannel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(white, white_predict, data_range=white.max() - white.min(), multichannel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros(shape=(150, 150))\n",
    "B = np.ones(shape=(150, 150))\n",
    "ssim(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pca = pca(latent_asian, latent_white, ethnie)\n",
    "cosinus_similarity(asian_flatten, asian_predict_flatten, white_flatten, white_predict_flatten)\n",
    "comparaison_dispersion(latent_asian, latent_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
